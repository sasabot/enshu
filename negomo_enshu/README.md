What?

This is a package for estimating human and robot interaction situation from visual observations.
The package estimates situations such as:
whether a human is reacting to a robot, trying to interact with a robot, or not interested in a robot.

The estimation runs on a ROS node(negotiation_model),
and requests for results can be passed via a bridging node(negomo_bridge). 
The package aims to handle various interaction emerging and ending situations, and allows different request patterns.
Therefore, at first glance, the package may look difficult to use with so many functionality.
We are trying the best to simplify usage.

----

Compile dependencies

1. git submodule update --init
2. cd StochHMM
3. ./configure
4. make

Install

1. negomo is a ROS package, it must be under your ROS workspace

Compile negomo

1. cd utils
2. ./setup.sh
3. catkin bt (or whatever way you compile a ROS package)

----

Creating Agreement Model from Actual Data

The agreement model is a Hidden Markov Model generated by defining
P(next_state | previous_state, action) and P(observation | state, action).
The following procedure explains how to generate models.

Generate models

1. cd utils
2. ./gen_header.sh initial70 during70

----

Running the Node

Make sure you have finished all steps above including "Creating Agreement Model from Actual Data".  
In your launch file, add following:
```
  <node name="negotiation_model" pkg="negomo" type="negotiation_model" output="screen">
     <rosparam file="$(find negomo)/config/negomo.yaml" command="load"/>
  </node>
  
  <node name="negomo_bridge" pkg="negomo" type="negomo_bridge" output="screen"/>
```

For the node to work, you must pass face recognition results to ```/negomo/sensor/face ``` topic.  
The topic takes a list of strings and the length should match the maximum number of detecting users
(max_targets in config/negomo.yaml).  
Valid string values are "noperson", "looktoward", "lookaway".

Or you may use [roboenvcv package](https://github.com/sasabot/roboenvcv) and edit
```examples/launch/negomo_from_sensors.launch ``` for extracting recognition results from RGB-D data.

Also, we recommend creating a log directory
1. roscd negomo
2. mkdir log

----

Examples

**simulated reacting robot example**  
This example uses keyboard input to simulate human head movement. Estimated results are shown in graph, and processing inside negomo is shown in a figure. Press 's' key to start simulation, and hold 'l' key to *look toward* robot. Robot will speak when reacted.
```
roslaunch negomo simulated_sample.launch
```
**simulated proacting robot example**  
This example is similar to above sample, except, this time, the robot tries to initiate conversation. When the graph is blue, negomo is estimating *reacting person* and when the graph is red, negomo is estimating *proacting person*. Robot will speak different phrases depending on estimated human state. Key inputs same as above example.
```
roslaunch negomo simulated_sample.launch mode:="simple_proactive"
```
**simulated planner example**  
This example uses keyboard input to simulate human head movement. To start planner,
```
roslaunch negomo simulate_planner_interaction_sample.launch
```
To show estimation results and how the planner is proceeding,
```
roslaunch negomo plot.launch use_input:=true
```
When a user interrupts planner, a UI will appear to enter task and values. For example, click handlePayment -> click anonymous -> enter ```*** ``` using GUI keyboard -> press enter on GUI keyboard -> press enter on task UI. This will simulate spoken task to robot.

----

Coding with negomo_lib

To connect codes with the negomo situation engine, you may use negomo_lib.
negomo_lib is an easy-to-use wrapper for calling the negomo situation engine.
negomo_lib is for C++ only. For other languages, please directly call services to the negomo_bridge node.

**setup**

In your C++ code under any package with negomo dependency, you may use negomo_lib by including
```
#include "negomo/NegomoLib.hh"
```
then, initiate your negomo situation engine
```
negomo_lib::NegomoBridgePtr engine;
engine.reset(new negomo_lib::NegomoBridge(nh, 2, 2));
```
negomo requires to register interaction-initiating actions, such as looking toward human or saying "Hey" to human.
Here is an example of an action,
```
void Proactive0(const std_msgs::Int32::ConstPtr &_msg) {
  auto pos_t = engine->getHeadPos(_msg->data);
  Eigen::Vector3d pos(std::get<0>(pos_t), std::get<1>(pos_t), std::get<2>(pos_t)); 
  // implement here: some action such as "looking" using head position of human id _msg->data
  engine->endAction();
};
```
then, register action with
```
engine->addProactiveAction(boost::bind(&Proactive0, _1));
```
Proactive actions are actions where the robot tries to catch human attention.
You may also register reactive actions where the robot responds to the human,
```
engine->addReactiveAction(boost::bind(&Reactive0, _1));
```
We recommend, setting two proactive actions without speech and with speech(registeration order is important), and also two reactive actions.
After registering actions, start the engine with
```
engine->start();
engine->startHeadposListener(); // listen to head pose, requires running a face detection node
```

**checking whether to respond to human**

Ask the engine with
```
auto response = engine->breakfrom();
```
If ```response.proceed == 1 && response.status_change == 0```, nothing happened.
If ```response.proceed == 1 && response.status_change == 1```, robot tried to react but was not necessary.
If ```response.proceed == 0```, robot should respond to human and start interacting.

**checking whether to bother human**

Ask the engine with
```
int proceed = engine->tryto();
```
If ```proceed == 1```, robot may bother arbitrary human (for a specific target, add id string to first argument).
If ```proceed == 0```, robot should respond to human instead.
If ```proceed < 0```, target was not found or some error occured.

**checking whether to end an interaction**

Make sure robot is in interaction state. The state is automatically handled by negomo but you may also force a during interaction state with
```
engine->force();
```
During an interaction state, you may ask the engine
```
int proceed = engine->peek(action_type);
```
whereas, action_type depends on speech content e.g. "reactive0"(no speech robot responding), "proactive0"(no speech robot asking), "reactive1"(with speech robot responding), "proactive1"(with speech robot asking). You may set "reactive0" as default but for more precise estimation, we recommend passing the action_type.  
If ```proceed == 0```, robot should finish interaction.

**forcefully ending an interaction**

The switch between initial interaction and during interaction is handled automatically in negomo.
However, if you wish to forcefully end an interaction, use
```
engine->away();
```

**checking engagement without robot actions**

Sometimes a robot may be busy doing a task and cannot conduct any interactive behaviors. In these cases, we might want to check whether a person is engaged without using any interactive behaviors. To do so, ask the engine with
```
bool proceed = engine->check(threshold);
```
where the threshold value should be between 0.0 and 1.0. With default settings, 0.0 indicates person is not interested and 1.0 indicates person is interested. (use options argument to change settings) The function returns ```true ``` if person is engaged and ```false ``` if not.

**experimental: checking human engagement during an interaction**

Short answer is to use ```check(threshold) ```. Long answer (for precise estimation using action type) is explained below.

If setting action_type and checking interaction state is done at same timing during an interaction,```peek(action_type) ``` is the right function to use. However, there are times when action_type wants to be set on background and interaction state wants to be checked elsewhere. In these situations, you may activate auto-peek mode in setup
```
engine.reset(new negomo_lib::NegomoBridge(nh, 2, 2, true));
```
then, use ```engine->force(); ``` to enter a during interaction state, or, if using automatically handled state, declare the following in the beginning of your during interaction code
```
engine->peekin();
```
To set action_type, send action type via topic ```/negomo/setduringaction ```, and check results using
```
int proceed = engine->peek();
```
If ```proceed == 0```, robot should finish interaction.
To end a during interaction state, use ```engine->away();``` or, if using automatically handled state, declare the following in the end of your during interaction code
```
engine->peekout();
```
